---
# https://github.com/rook/rook/blob/master/deploy/charts/rook-ceph-cluster/values.yaml
operatorNamespace: {{ rook_namespace }}

#clusterName: "{{ k8s_cluster_name }}"

toolbox:
  enabled: false
monitoring:
  enabled: false
  # createPrometheusRules: true
  # rulesNamespaceOverride: monitoring

pspEnable: false

# This config is converged cluster where all Ceph daemons are running locally
cephClusterSpec:
  cephVersion:
    image: quay.io/ceph/ceph:v{{ ceph_version }}
    allowUnsupported: false

  # Important: clear this folder on the host if you recreate the cluster
  dataDirHostPath: /var/lib/rook

  mon:
    count: 3
    allowMultiplePerNode: false
  mgr:
    count: 2
    allowMultiplePerNode: false
    modules:
      # "dashboard" and "monitoring" modules are enabled by other settings
      - name: pg_autoscaler
        enabled: true
  dashboard:
    enabled: true
    ssl: false
  network:
    connections:
      encryption:
        enabled: false
      compression:
        enabled: false
      requireMsgr2: false
    # FIXME: figure out IPv6 settings -
    # Currently setting these will cause a loss of communications to the pgs:
    # ipFamily: "IPv6"
    # dualStack: true

  crashCollector:
    disable: false
    #daysToRetain: 30

  logCollector:
    enabled: true
    periodicity: daily  # one of: hourly, daily, weekly, monthly
    maxLogSize: 50M

  cleanupPolicy:
    # Since cluster cleanup is destructive to data, confirmation is required.
    # To destroy all Rook data on hosts during uninstall, confirmation must
    # be set to "yes-really-destroy-data"
    confirmation: ""
    sanitizeDisks:
      method: quick
      dataSource: zero  # random takes much longer
      iteration: 1
    # delete PVs before deleting the cluster
    allowUninstallWithVolumes: false

  placement:
    all:
      tolerations:
        - effect: NoSchedule
          key: node-role.kubernetes.io/control-plane
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
            - matchExpressions:
              - key: "{{ lan_domain }}/node-role"
                operator: In
                values:
                  - storage-node

  # Automatically remove OSDs that are out and are safe to destroy.
  removeOSDsIfOutAndSafeToRemove: false

  storage:
    useAllNodes: false
    useAllDevices: false
    # deviceFilter: sda
    config:
      # specify a non-default root label for the CRUSH map:
      crushRoot: "{{ lan_domain }}-ceph-root"
      osdsPerDevice: "1" # Value can be overridden at node or device level
      # a non-rotational storage ceph-volume uses as block db dev of bluestore
      # metadataDevice: "md0"
      # encryptedDevice: "true"

    {% if ceph_storage is defined -%}
    nodes:
      {% for node in ceph_storage -%}
      - name: "{{ node.name }}"
        {% if node.devices is defined -%}
        devices:
          {% for dev in node.devices -%}
          - name: "{{ dev.name }}"
            config:
              osdsPerDevice: "{{ dev.osds }}"
          {% endfor %}
        {%- elif node.deviceFilter is defined -%}
        deviceFilter: {{ node.deviceFilter }}
        {%- endif %}

      {% endfor %}
    {% endif %}

  # Configure management of daemon disruptions during upgrade or fencing.
  disruptionManagement:
    managePodBudgets: true
    osdMaintenanceTimeout: 30
    # wait until healthy
    pgHealthCheckTimeout: 0

ingress:
  dashboard:
    host:
      name: ceph.{{ lan_domain }}
      path: /
    ingressClassName: traefik
    tls:
      - hosts:
          - ceph.{{ lan_domain }}
        secretName: {{ lan_domain_cert }}

cephBlockPools:
  - name: ceph-blockpool
    spec:
      failureDomain: host
      replicated:
        size: 3
    storageClass:
      enabled: true
      name: ceph-block
      isDefault: true
      reclaimPolicy: Delete
      allowVolumeExpansion: true
      volumeBindingMode: "Immediate"

      parameters:
        # RBD image format - defaults to "2"
        imageFormat: "2"
        imageFeatures: layering
        #,fast-diff,object-map,deep-flatten,exclusive-lock

        # These secrets contain Ceph admin credentials.
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: "{{ rook_namespace }}"
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: "{{ rook_namespace }}"
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
        csi.storage.k8s.io/node-stage-secret-namespace: "{{ rook_namespace }}"

        # Do not use xfs when the volume is on the same node as osds
        csi.storage.k8s.io/fstype: ext4

cephFileSystems:
  - name: ceph-filesystem
    spec:
      metadataPool:
        replicated:
          size: 3
      dataPools:
        - failureDomain: host
          replicated:
            size: 3
          name: "{{ lan_domain }}-data0"
      metadataServer:
        activeCount: 1
        activeStandby: true
        priorityClassName: system-cluster-critical
    storageClass:
      enabled: true
      isDefault: false
      name: cephfs
      pool: "{{ lan_domain }}-data0"
      reclaimPolicy: Retain
      allowVolumeExpansion: true
      volumeBindingMode: "Immediate"
      parameters:
        # The secrets contain Ceph admin credentials.
        csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/provisioner-secret-namespace: "{{ rook_namespace }}"
        csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
        csi.storage.k8s.io/controller-expand-secret-namespace: "{{ rook_namespace }}"
        csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
        csi.storage.k8s.io/node-stage-secret-namespace: "{{ rook_namespace }}"

        # Do not use xfs in when volume is on same node as osds.
        csi.storage.k8s.io/fstype: ext4

cephFileSystemVolumeSnapshotClass:
  enabled: false
  name: cephfs
  isDefault: true
  deletionPolicy: Delete

cephBlockPoolsVolumeSnapshotClass:
  enabled: false
  name: ceph-block
  isDefault: false
  deletionPolicy: Delete

cephObjectStores:
  - name: ceph-objectstore
    spec:
      metadataPool:
        failureDomain: host
        replicated:
          size: 3
      dataPool:
        failureDomain: host
        replicated:
          size: 3
      preservePoolsOnDelete: true
      gateway:
        port: 80
        # securePort: 443
        # sslCertificateRef:
        instances: 1
        priorityClassName: system-cluster-critical
    storageClass:
      enabled: true
      name: ceph-bucket
      reclaimPolicy: Delete
      volumeBindingMode: "Immediate"
      parameters:
        region: us-east-1
    ingress:
      enabled: true
      host:
        name: s3.{{ lan_domain }}
        path: /
      ingressClassName: traefik
      tls:
        - hosts:
            - ceph.{{ lan_domain }}
          secretName: {{ lan_domain_cert }}
